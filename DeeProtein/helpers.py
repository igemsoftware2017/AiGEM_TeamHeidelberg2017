import matplotlib
matplotlib.use('Agg')
import tensorflow as tf
import numpy as np
import re, os
import random
import math
import json
import tarfile
import matplotlib.pyplot as plt
import sklearn.metrics
from seaborn import barplot, set_style
from sklearn.preprocessing import OneHotEncoder
from collections import OrderedDict
from matplotlib import font_manager


class OptionHandler():
    """This Class holds all the Options from the config.JSON. The Attributes of this class are directly accessed
    by DeeProtein, the BatchGenerator, the ROCtracker and the TFrecordsgenerator.

    Attributes:
      config: A `dict` holding all the global paramters. This is read from config.JSON.
      _name: A `str` holding the model name.
      _thresholdnum: `int32`, nr of thresholds to use to calculate the ROC/PR metrics.
      _gpu: `int32`, which gpu to use in multiGPU context.
      _allowsoftplacement: `str` if 'True' the Variables can be moved to CPU.
      _numepochs: `int32` the number of epochs to calculate. DEPREACTED. Use numsteps instead.
      _numsteps: `int32` the number of steps to run the model.
      _embeddingdim: `int32` the dimension of the protein embedding generated by Deeprotein.generate_embedding().
      _depth: `int32` the depth the nr of amino acids. Defaults to 20.
      _structuredims: `int32` the nr of structural elements to consider in the structure embedding.
      _traindata: `str` the path to the traindataset.
      _validdata: `str` the path to the validdataset.
      _batchesdir: `str` the path to the examples directory.
      _inferencemode: `str` if 'True' model is initialized in inference mode.
      _labels: `str` the type od labels to use must be one of ['EC' or 'GO']. Until now only 'GO' is fully implemented.
      _nclasses: `int32` the number of classes to consider. This number must match the line number in the EC_file.
      _classbalancing: `str` if 'True' the classes are weighted for their size/their importance during training.
      _maxclassinbalance: `int32` the maximum weight a class can obtain.
      _dropoutrate: `float32` the dropout to assign to fully connected layers.
      _learningrate: `float32` the learningrate.
      _epsilon: `float32` the epsilon parameter of the Adam optimizer.
      _batchsize: `int32` the bacthsize to apply in trainig and validation mode.
      _batchgenmode: `str` currently only one option is available: 'one_hot_padded'
      _windowlength: `int32` the length of the window to apply on a sequence.
      _minlength: `int32` the minlength a sequence must have to be included in the dataset.
      _numthreads: `int32` the number of threads to use in the inputpipeline.
      _restorepath: `int32` the path from which to restore the model.
      _restore: `str` if 'True' the model is restored from the path specified in the restorepath.
      _debug: `str` if 'True' the model is initialized in the debug mode.
      _ec_file: `str` the path to the EC_file, holding all labels and the label sizes.
      _summariesdir: `str` the directory where to store the model and to write the summaries.
    """
    def __init__(self, config_dict):
        self.config = config_dict
        self._name = config_dict['model_name']
        self._thresholdnum = config_dict['threshold_num']
        self._gpu = config_dict['gpu']
        self._allowsoftplacement = config_dict['allow_softplacement']
        self._numepochs = config_dict['num_epochs']
        self._numsteps = config_dict['num_steps']
        self._depth = config_dict['depth']
        self._structuredims = config_dict['structure_dims']
        self._traindata = config_dict['train_data']
        self._validdata = config_dict['valid_data']
        self._batchesdir = config_dict['batches_dir']
        self._inferencemode = True if config_dict['inference_mode'] == 'True' else False
        self._labels = config_dict['labels']
        self._nclasses = config_dict['n_classes']
        self._classbalancing = True if config_dict['class_balancing'] == 'True' else False
        self._maxclassinbalance = config_dict['maxclass_inbalance']
        self._dropoutrate = config_dict['dropoutrate']
        self._learningrate = config_dict['learning_rate']
        self._epsilon = config_dict['epsilon']
        self._batchsize = config_dict['batch_size']
        self._batchgenmode = config_dict['batchgen_mode']
        self._windowlength = config_dict['window_length']
        self._minlength = config_dict['min_length']
        self._numthreads = config_dict['num_threads'] #TODO ASSERT THIS NUMBER!!!!!!!!
        self._restorepath = config_dict['restore_path']
        self._restore = True if config_dict['restore'] == 'True' else False
        self._debug = True if config_dict['debug'] == 'True' else False
        self._ecfile = config_dict['EC_file']
        self._summariesdir = config_dict['summaries_dir'] # for tensorboard
        self._summariesdir = self._summariesdir + \
                           '_{l}_{n}_{w}_{g}_{b}_{lr}_{e}'.format(g=self._batchgenmode,
                                                                  w=self._windowlength,
                                                                  n=self._nclasses,
                                                                  b=self._batchsize,
                                                                  lr=self._learningrate,
                                                                  e=self._epsilon,
                                                                  l=self._labels)
        if not os.path.exists(self._summariesdir):
            os.makedirs(self._summariesdir)
        if not os.path.exists(self._batchesdir):
            os.makedirs(self._batchesdir)

    def write_dict(self):
        """Store the config_dict on disc in the save_dir.
        """
        with open(os.path.join(self._summariesdir, 'config_dict.JSON'), "w") as config_dict:
            json.dump(self.config, config_dict)


class RocTracker():
    """This class calculates comprehensive metrics for the validation of the performace of DeeProtein.
    The calculated metrics include Area under the ROC-Curve and AUC under the Precision/recall-curve.

    Attributes:
    _opts: A `helpers.OptionHandler` defining the global preferences.
    metrics_path: `str` the path to the metrics folder.
    metrics_file: `str` file to the metrics.csv where the most recent metrics are stored.
    roc_score: `Array` holding the logits of the model for each validation batch.
    roc_labels: `Array` holding the labels of the model for each validation batch.
    pred_positives_sum: `Array` holding the positive predictions for each class.
    actual_positives_sum: `Array` holding the condition positives predictions for each class.
    true_positives_sum: `Array` holding the true positives predictions for each class.
    num_calculations: `int32` as a counter.
    """
    def __init__(self, optionhandler):
        self._opts = optionhandler
        self.metrics_path = os.path.join(self._opts._summariesdir, 'metrics')
        if not os.path.exists(self.metrics_path):
            os.mkdir(self.metrics_path)
        self.metrics_file = open(os.path.join(self.metrics_path, 'metrics.csv'), "w")
        self.roc_score = []
        self.roc_labels = []
        self.pred_positives_sum = np.zeros(self._opts._nclasses)
        self.actual_positives_sum = np.zeros(self._opts._nclasses)
        self.true_positive_sum = np.zeros(self._opts._nclasses)
        self.num_calculations = 0

        try:
            plt.style.use(json.load(
                open('/net/data.isilon/igem/2017/scripts/clonedDeeProtein/DeeProtein/style.json', 'r')))
            self.font = font_manager.FontProperties(
                fname='/net/data.isilon/igem/2017/data/cache/fonts/JosefinSans-Regular.tff')
            self.monospaced = font_manager.FontProperties(
                fname='/net/data.isilon/igem/2017/data/cache/fonts/DroidSansMono-Regular.ttf')
        except:
            pass

    def update(self, sigmoid_logits, true_labels):
        """Update the ROC tracker, with the predictions on one batch made during validation.

        Args:
          sigmoid_logits: `np.Array` and 2D arrray holding the sigmoid logits for the validation batch.
          true_labels: `np.Array` and 2D arrray holding the true labels for the validation batch.
        """
        # threshold this thing
        # we consider a class "predicted" if it's sigmoid activation is higher than 0.5 (predicted labels)
        batch_predicted_labels = np.greater(sigmoid_logits, 0.5)
        batch_predicted_labels = batch_predicted_labels.astype(float)


        batch_pred_pos = np.sum(batch_predicted_labels, axis=0) #sum up along the batch dim, keep the channels
        batch_actual_pos = np.sum(true_labels, axis=0) #sum up along the batch dim, keep the channels
        # calculate the true positives:
        batch_true_pos = np.sum(np.multiply(batch_predicted_labels, true_labels), axis=0)

        # and update the counts
        self.pred_positives_sum += batch_pred_pos #what the model said
        self.actual_positives_sum += batch_actual_pos #what the labels say
        self.true_positive_sum += batch_true_pos # where labels and model predictions>0.5 match

        assert len(self.true_positive_sum) == self._opts._nclasses

        # add the predictions to the roc_score tracker
        self.roc_score.append(sigmoid_logits)
        self.roc_labels.append(true_labels)

    def calc_and_save(self, logfile):
        """Calculate the ROC curve with AUC value for the collected test values (roc_scores, roc_labels).
        Writes everything to files, plots curves and resets the Counters afterwards.

        Args:
          logfile: `file-object` the logfile of the DeeProtein model.
        """
        self.metrics_file = open(os.path.join(self.metrics_path, 'metrics.csv'), "w")

        self.num_calculations += 1

        # concat score and labels along the batchdim -> a giant test batch
        self.roc_score = np.concatenate(self.roc_score, axis=0)
        self.roc_labels = np.concatenate(self.roc_labels, axis=0)

        # get the total number of seqs we tested on:
        logfile.write('[*] Calculating metrics\n')
        test_set_size = self.roc_labels.shape[0]

        # do the calculations
        fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true=np.reshape(self.roc_labels, (-1)),
                                                         y_score=np.reshape(self.roc_score, (-1)))
        auc = sklearn.metrics.auc(fpr, tpr)

        precision_arr, recall_arr, thresholds = sklearn.metrics.precision_recall_curve(
            y_true=np.reshape(self.roc_labels, (-1)), probas_pred=np.reshape(self.roc_score, (-1)))

        # write get the max, min and avg scores for each class:
        # determine the scores for the labels
        scores = self.roc_score * self.roc_labels

        mean_scores = np.mean(scores, axis=0)
        assert mean_scores.shape[0] == self._opts._nclasses
        max_scores = np.amax(scores, axis=0)
        assert max_scores.shape[0] == self._opts._nclasses
        min_scores = np.amin(scores, axis=0)
        assert min_scores.shape[0] == self._opts._nclasses

        self.metrics_file.write(str(mean_scores) + '\n')
        self.metrics_file.write(str(max_scores) + '\n')
        self.metrics_file.write(str(min_scores) + '\n')

        self.metrics_file.close()

        # get printable metrics (for log file):
        # where predPositives_sum == 0, tp_sum is also 0
        precision_class = self.true_positive_sum / np.maximum(1, self.pred_positives_sum)
        # where actualPositives_sum == 0, tp_sum is also 0
        recall_class = self.true_positive_sum / np.maximum(1, self.actual_positives_sum)
        precision = np.sum(self.true_positive_sum) / np.sum(self.pred_positives_sum)
        recall = np.sum(self.true_positive_sum) / np.sum(self.actual_positives_sum)
        f1 = 2*precision*recall / (precision + recall)
        logfile.write("[*] Tested on %d seqs, "
                      "precision %.2f%%, "
                      "recall %.2f%%, "
                      "F1 %.2f%%\n" % (test_set_size, precision, recall, f1))
        logfile.flush()

        #plot ROC:
        self.plot_simple_curve(x=fpr, y=tpr, title=self._opts._name + '_ROC_curve',
                          legend=self._opts._name + ' (AUC = %0.4f)' % auc,
                          xname='False positive rate', yname='True positive rate',
                          filename=os.path.join(self.metrics_path, self._opts._name +
                                                '.roc_%d' % self.num_calculations))


        # PR curve
        self.plot_simple_curve(x=recall_arr, y=precision_arr,
                          title=self._opts._name + ' PR curve', legend=self._opts._name,
                          xname='Recall', yname='Precision',
                          filename=os.path.join(self.metrics_path, self._opts._name +
                                                '.precision_%d' % self.num_calculations),
                               include_linear=False)

        # reset the stats-collectors:
        self.roc_score = []
        self.roc_labels = []
        self.pred_positives_sum = np.zeros(self._opts._nclasses)
        self.actual_positives_sum = np.zeros(self._opts._nclasses)

        logfile.write('[*] Done testing.\n')

    def plot_simple_curve(self, x, y, title, legend, xname, yname, filename, include_linear=True, iGEM_style=True):
        """Plots simple curve in the iGEM style if wanted.

        Args:
          x: `Array1d`, what to plot on the x-Axis.
          y: `Array1d` what to plot on the y-Axis.
          title: `str`, the title.
          legend:`str`, the legend.
          xname: `str`, the name of the x axis.
          yname: `str`, the name of the y axis.
          filename: `str`, path to the file where to save the plot.
          include_linear: `bool`, whether to plot a linear line with slope=1.
          iGEM_style: `bool`, whether to plot in the iGEM-Heidelberg style layout.
        """
        plt.ioff()
        fig = plt.figure(1, figsize=(5, 5), dpi=200)
        plt.plot(x, y, color='#005493', lw=2, label=legend)
        if include_linear:
            plt.plot([0, 1], [0, 1], color='#B9B9B9', lw=2, linestyle="--")
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        if iGEM_style:
            plt.title(title, fontproperties=self.font)
            plt.xlabel(xname, fontproperties=self.font)
            plt.ylabel(yname, fontproperties=self.font)
            plt.legend(loc="lower right", prop=self.font)
        else:
            plt.title(title)
            plt.xlabel(xname)
            plt.ylabel(yname)
            plt.legend(loc="lower right")

        plt.savefig(filename+".svg")
        plt.savefig(filename+".png")
        plt.close(fig)


def _count_lines(file_path):
    """Count lines in a file.
    A line counter. Counts the lines in given file with counter count counts.

    Args:
      file_path: `str` path to file where to count the lines.
    """
    count = 0
    with open(file_path, "r") as fobj:
        for line in fobj:
            count += 1
    return count


class StratifiedCounterDict(dict):
    def __missing__(self, key):
        self[key] = {'tp': 0,
                     'pred_p': 0,
                     }
        return self[key]


class BatchGenerator():
    """Batchgenerator of the DeeProtein model.
    This Class generates the life batches for inference and holds important information such as the class_dict
    and the embedding_dict.
    It further can be used to perform tests on a model by generating batches of garbage sequences or batches of
    sequences for inference.

    Attributes:
        _opts: A `helpers.OptionHandler` holding the global options for the module.
        mode: A `str`. Has to be one of ['one_hot_padded']
        inferencedata: A `fileObject` the file from which to infer.
        traindata: A `fileObject` the file from which to train.
        validdata: A `fileObject` the file from which to evaluate.
        AA_to_id: A `dict` mapping single letter AAs to integers.
        id_to_AA: A `dict` the reverse of AA_to_id.
        class_dict: An `OrderedDict` mapping `str` labels to integers. The order is kept from the EC_file specifier in
          the config_dict.
        id_to_class: An `OrderedDict` the reverse class_dict.
        embedding_dict: An `OrderedDict` mapping sequence names to the index in the protein embedding.
        garbage_percentage: A `float32` defining the amount of Garbage to incorporate in the dataset.
        garbage_count: A `int32` used to count the nr of garbage that has been written.
        eval_batch_nr: A `int32`, the nr of batches needed for complete evaluation of the valid set.
        batches_per_file: A `int32`, defines the nr of batches that is included in a tf.Records file.
        epochs: A `int32` defining the nr of epochs to train.
        curr_epoch: A `int32` counter for the epcohs.
        label_enc: A `sklearn.preprocessing.OneHotEncoder` used to encode the labels.
        AA_enc: A `sklearn.preprocessing.OneHotEncoder` used to encode the AA letters.
    """
    def __init__(self, optionhandler):
        self._opts = optionhandler
        self.mode = self._opts._batchgenmode # one of ['window', 'bigbox', 'dynamic']
        self.traindata = open(self._opts._traindata, 'r')
        self.validdata = open(self._opts._validdata, 'r')
        self.AA_to_id = {}
        self.id_to_AA = {}
        self.class_dict = OrderedDict()
        self.id_to_class = OrderedDict()
        self._get_class_dict()
        self.embedding_dict = OrderedDict()
        # determine the number of batches for eval from lines in the validdata and the garbagepercentage
        self.garbage_percentage = 0.2
        self.garbage_count = 0 # a counter for generated garbage sequences
        self.eval_batch_nr = int(_count_lines(self._opts._validdata) * (1 + self.garbage_percentage) //
                              self._opts._batchsize)
        print('Initialized Batchgen with   batchsize: %d,   numeval_batches: %d at'
              '                            garbage_percentage: %f' % (self._opts._batchsize,
                                                                      self.eval_batch_nr,
                                                                      self.garbage_percentage))
        self.batches_per_file = 10000
        self.epochs = 2000
        self.curr_epoch = 0
        self.label_enc = OneHotEncoder(n_values=self._opts._nclasses, sparse=False)
        self.AA_enc = 'where we put the encoder for the AAs'

        if self.mode.startswith('one_hot'):
            AAs = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',
                   'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y',]

            self.AA_enc = OneHotEncoder(n_values=self._opts._depth, sparse=False)

            if 'physchem' in self.mode: #this is to be implemented.
                _hydro = [1.8, 2.5, -3.5, -3.5, 2.8,
                          -0.4, -3.2, 4.5, -3.9, 3.8,
                          1.9, -3.5, -1.6, -3.5, -4.5,
                          -0.8, -0.7, 4.2, -0.9, -1.3]
                _molarweight = [89.094, 121.154, 133.104, 147.131, 165.192,
                                75.067, 155.156, 131.175, 146.189, 131.175,
                                149.208, 132.119, 115.132, 146.146, 174.203,
                                105.093, 119.119, 117.148, 204.228, 181.191]
                _is_polar = lambda aa: 1 if aa in ['DEHKNQRSTY'] else 0
                _is_aromatic = lambda aa: 1 if aa in ['FWY'] else 0
                _has_hydroxyl = lambda aa: 1 if aa in ['ST'] else 0 #should we add TYR??
                _has_sulfur = lambda aa: 1 if aa in ['CM'] else 0

                for i, aa in enumerate(AAs):
                    self.AA_to_id[aa]  = {'id': len(self.AA_to_id),
                                          'hydro': _hydro[i],
                                          'molweight': _molarweight[i],
                                          'pol': _is_polar(aa),
                                          'arom': _is_aromatic(aa),
                                          'sulf': _has_sulfur(aa),
                                          'OH': _has_hydroxyl(aa)}
            else:
                for aa in AAs:
                    self.AA_to_id[aa] = len(self.AA_to_id)
                # get the inverse:
                self.id_to_AA = {}
                for aa, id in self.AA_to_id.items():
                    self.id_to_AA[id] = aa
                self.id_to_AA[42] = '_'

    def _get_class_dict(self):
        """Generate the class-dict.
        The class dict stores the link between the index in the one-hot encoding and the class.
        """
        with open(self._opts._ecfile, "r") as ec_fobj:
            for line in ec_fobj:
                fields = line.strip().split()
                if fields[1].endswith('.csv'): 
                    fields[1] = fields[1].rstrip('.csv')

                if self._opts._labels == 'EC':
                    self.class_dict[fields[1]] = {'id': len(self.class_dict),
                                                  'size': int(fields[0]),
                                                  }
                if self._opts._labels == 'GO':
                    self.class_dict[fields[1].split('_')[1]] = {'id': len(self.class_dict),
                                                                'size': int(fields[0]),
                                                                }

        # get a reverse dict:
        for key in self.class_dict.keys():
            self.id_to_class[self.class_dict[key]['id']] = key

    def _update_embedding_dict(self, name, labels):
        """Helper function for the DeeProtein.generate_embedding().
        Update the embedding dict for new entries. This is used on the fly as we perform inference batchgen.

        Args:
          name: A `str`, name of the new token/protein
          labels: A `list` of strings which labels to assign to the new token
        """
        if len(self.embedding_dict) == 0:
            # add UNK token the first time this method is called
            self.embedding_dict['UNK'] = {}
            self.embedding_dict['UNK']['labels'] = ['UNK']
            self.embedding_dict['UNK']['id'] = 0 # we save 0 for the UNK token

        # check if the key (= name) is already in the dict:
        if name not in self.embedding_dict:
            assert len(self.embedding_dict) > 0
            self.embedding_dict[name] = {}
            self.embedding_dict[name]['labels'] = labels
            self.embedding_dict[name]['id'] = len(self.embedding_dict)
        else:
            # get a new entry to the dict:
            name_matches = [k for k in self.embedding_dict.keys() if k.startswith(name)]
            new_name = name + str(len(name_matches))
            assert len(self.embedding_dict) > 0
            self.embedding_dict[new_name] = {}
            self.embedding_dict[new_name]['labels'] = labels
            self.embedding_dict[new_name]['id'] = len(self.embedding_dict)

    def _csv_EC_decoder(self, in_csv, encoded_labels=True):
        """Helper function for _process_csv().
        This function takes an in csv obj and iterates over the lines.

        Args:
          in_csv: `fileobject`, the incsv file object.
          encoded_labels: `bool`, wheter to return one-hot encoded labels or the label as string (optional).
            Defaults to True.

        Returns:
          name: A `str` holding the name of the sample sequence.
          seq: A `str` holding the sequence.
          seq: A `str` holding the label(s).
        """
        assert self.mode == 'one_hot_padded', 'mode must be one_hot_padded'
        line = in_csv.readline()
        fields = line.strip().split(';')
        name = fields[0]
        seq = fields[1]
        if fields[2].endswith('.csv'): #TODO assert this
            fields[2] = fields[2].rstrip('.csv')
        if self._opts._labels == 'EC':
            EC_str = fields[2] #TODO assert this
            if encoded_labels:
                EC_CLASS = 0 if self._opts._inferencemode else self.class_dict[EC_str]['id']
                label = [[EC_CLASS]] # we need a 2D array
            else:
                label = EC_str
        elif self._opts._labels == 'GO':
            GO_str = fields[2] #TODO assert this
            GOs = 0 if self._opts._inferencemode else fields[2].split(',') #TODO assert this
            if encoded_labels:
                label = [[self.class_dict[go]['id']] for go in GOs] # returns a 2D array
            else:
                label = GOs
        return name, seq, label

    def _seq2tensor(self, seq):
        """Helper function for _process_csv().
        Takes a sequence and encodes it onehot.

        Args:
          seq: `str`, The sequence to encode

        Returns:
          padded_seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos.
          length: A `Tensor` holding the length.
        """
        if self.mode == 'one_hot_padded':
            # first check if the sequence fits in the box:
            if len(seq) <= self._opts._windowlength:
                seq_matrix = np.ndarray(shape=(len(seq)), dtype=np.int32)
            # if sequence does not fit we clip it:
            else:
                seq_matrix = np.ndarray(shape=(self._opts._windowlength), dtype=np.int32)
            for i in range(len(seq_matrix)):
                seq_matrix[i] = self.AA_to_id[seq[i]]
            start_pos = 0 #because our sequence sits at the beginning of the box
            length = len(seq_matrix)  #true length (1 based)
            # now encode the sequence in one-hot
            oh_seq_matrix = np.reshape(self.AA_enc.fit_transform(np.reshape(seq_matrix, (1, -1))),
                                       (len(seq_matrix), 20))
            # pad the sequence to the boxsize:
            npad = ((0, self._opts._windowlength-length), (0, 0))
            padded_seq_matrix = np.pad(oh_seq_matrix, pad_width=npad, mode='constant', constant_values=0)
            padded_seq_matrix = np.transpose(padded_seq_matrix)
            del oh_seq_matrix, seq_matrix

            return padded_seq_matrix, start_pos, length #true length 1 based
        else:
            print("Error: MODE must be of 'one_hot_padded'")

    def _encode_single_seq(self, seq, desired_label=None):
        """Encode a single sequence one-hot.
        This funciton is thought to be used in the inference mode.

        Args:
          seq: A `str` holding the sequence to be encoded.
          desired_label: A `str` holding the label to assign to this sequence (optional).

        Returns:
          oh_label: A `Tensor` holding the one_hot encoded label. This is only returned if a desired label was passed.
          seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos. This is only returned if a desired label was passed.
          length: A `Tensor` holding the length. This is only returned if a desired label was passed.
        """
        seq_matrix, start_pos, length = self._seq2tensor(seq)
        # look up the label in the class_dict:
        if desired_label:
            desired_label_ID = self.class_dict[desired_label]['id']

            # encode label one_hot:
            oh_label = self.label_enc.fit_transform([[desired_label_ID]]) # of shape [1, n_classes]
            return oh_label, seq_matrix, start_pos, length

        else:
            return seq_matrix

    def _process_csv(self, queue, return_name=True, encode_labels=True):
        """Iterate over a dataset.csv and process the samples.

        Takes the path to a file and processes the samples a sample at a time whenever the function is called.

        Args:
          queue: `str` the file_path to the dataset.csv. The csv file should be in the form of:
            name,seq,labels.
          return_name: `bool` whether to return the sequence name or not. Defaults to True.
          encode_labels: `bool` whether to return the encoded labels or the raw str. Defaults to True.

        Returns:
          Depending on return_name either (if true):
          name: A `str` holding the sample name.
          label: A `Tensor` holding the one-hot encoded labels (depending on encode_labels, otherwise raw labels).
          seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos of the sequence in the window (defaults to 0).
          end_pos:  A `Tensor` holding the end pos of the sequence in the window.

          OR
          label: A `Tensor` holding the one-hot encoded labels (depending on encode_labels, otherwise raw labels).
          seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos of the sequence in the window (defaults to 0).
          end_pos:  A `Tensor` holding the end pos of the sequence in the window.
        """
        name, seq, label = self._csv_EC_decoder(queue, encoded_labels=encode_labels)
        seq_matrix, start_pos, end_pos = self._seq2tensor(seq)
        if return_name:
            return name, label, seq_matrix, start_pos, end_pos
        else:
            return label, seq_matrix, start_pos, end_pos

    def generate_garbage_sequence(self, return_name=False):
        """Generate a garbage sequence.
        Generates a sequence full of garbage, e.g. a obviously non functional sequence. Used to test the robustness of
        the model precision.

        Args:
          return_name: A `bool`, if True return the garbage sequene along wih a generated name.

        Returns:
          name: A `str` holding the sample name (if return_name is True).
          padded_seq: A `Tensor` holding the one-hot encoded and padded (to windowlength) garbage sequence.
          label: A `Tensor` holding the one-hot encoded labels (depending on encode_labels, otherwise raw labels).
            The labels of a garbage sequence are a zeros Tensor.
          garbage_label: A `Tensor` holding the garbage label (binary, either 1 for garbage or 0 for valid sequence).
        """
        modes = ['complete_random', 'pattern', 'same']
        mode = modes[random.randint(0, 2)]
        self.garbage_count += 1

        # get the length of the protein
        length = random.randint(175, self._opts._windowlength-10) #enforce padding

        if mode == 'pattern':
            #print('pattern')
            # Generate a repetitive pattern of 5 AminoAcids to generate the prot
            # get a random nr of AAs to generate the pattern:
            AA_nr = random.randint(2, 5)
            # get an index for each AA in AA_nr
            idxs = []
            for aa in range(AA_nr):
                idx_found = False
                while not idx_found:
                    aa_idx = random.randint(0, 19)
                    if not aa_idx in idxs:
                        idxs.append(aa_idx)
                        idx_found = True
            reps = math.ceil(length/AA_nr)
            seq = reps * idxs
            length = len(seq)

        elif mode == 'complete_random':
            # print('complete_random')
            seq = []
            for aa in range(length):
                # get an idx for every pos in length:
                idx = random.randint(0, 19)
                seq.append(idx)

        elif mode == 'same':
            # print('ONE')
            AA = random.randint(0, 19)
            seq = length * [AA]

        label = np.zeros([self._opts._nclasses])
        label = np.expand_dims(label, axis=0)
        garbage_label = np.asarray([1])
        garbage_label = np.expand_dims(garbage_label, axis=0)
        oh_seq_matrix = np.reshape(self.AA_enc.fit_transform(np.reshape(seq, (1, -1))), (len(seq), 20))
        # pad the sequence to the boxsize:
        npad = ((0, self._opts._windowlength-length), (0, 0))
        padded_seq_matrix = np.pad(oh_seq_matrix, pad_width=npad, mode='constant', constant_values=0)
        padded_seq = np.transpose(padded_seq_matrix)
        if return_name:
            # return a sequence ID to identify the generated sequence
            # generate a "random" name
            name = 'g%d' % self.garbage_count
            return name, padded_seq, label, garbage_label
        else:
            return padded_seq, label, garbage_label

    def generate_random_data_batch(self):
        """Generates a batch consisting solely of random data.

        Generate a batch full of random data drawn form a normal distribution.

        Returns:
          seq_tensor_batch: A `Tensor`, [batchsize, 20, windowlength, 1] holding the random data batch.
          onehot_labelled_batch: A `Tensor`, [batchsize, n_classes] holding the random labels.
        """
        seq_tensor_batch = tf.random_normal([self._opts._batchsize, self._opts._embeddingdim,
                                             self._opts._windowlength, 1])

        label_batch = [np.random.randint(1,self._opts._nclasses) for _ in range(self._opts._batchsize)]
        index_batch = [tf.constant(label) for label in label_batch]
        label_tensor = tf.stack(index_batch)
        onehot_labelled_batch = tf.one_hot(indices=tf.cast(label_tensor, tf.int32),
                                           depth=self._opts._nclasses)
        return seq_tensor_batch, onehot_labelled_batch

    def generate_batch(self, is_train):
        """Generate a batch for the feed_dict pipeline.

        This function is depracted. Please use the input pipeline along with the TFrecordsgenrator
        training sequence input.

        Args:
          is_train: A `bool` from which dataset (train/valid) to draw the samples.

        Returns:
          batch: A `Tensor`, [batchsize, 20, windowlength, 1] holding the inference data batch.
          label_batch: A `Tensor`, [batchsize, n_classes] holding the labels.
          positions: A `Tensor` [batchsize, start, end], where the start/end position of a sequence is.
        """
        seq_tensors = []
        label_batch = []
        positions = np.ndarray([self._opts._batchsize, 2])
        lengths = np.ndarray([self._opts._batchsize])
        if is_train:
            in_csv = self.traindata
        else:
            in_csv = self.validdata
        for i in range(self._opts._batchsize):
            try:
                """ Note that this is not shuffled! """
                ECclass, seq_tensor, start_pos, end_pos = self._process_csv(in_csv, return_name=False,
                                                                            encode_labels=True)
                label_batch.append(ECclass)
                seq_tensors.append(seq_tensor)
            except IndexError: # catches error from csv_decoder
                # reopen the file:
                in_csv.close()
                # TODO: implement file shuffling when we reopen the file
                if is_train:
                    self.traindata = open(self._opts._traindata, 'r')
                    in_csv = self.traindata
                else:
                    self.validdata = open(self._opts._validdata, 'r')
                    in_csv = self.validdata
                """ redo """
                ECclass, seq_tensor, start_pos, end_pos = self._process_csv(in_csv, return_name=False,
                                                                            encode_labels=True)
                label_batch.append(ECclass)
                seq_tensors.append(seq_tensor)

                positions[i, 0] = start_pos
                positions[i, 1] = end_pos
                lengths[i] = end_pos

        batch = np.stack(seq_tensors, axis=0)

        return batch, label_batch, positions

    def generate_binary_batch(self):
        """Generate a binary batch for training protein activity.

        This function requires a special input dataset, where the labels are already encoded as their target float.
        Thus this function does NOT use the GO-file nor the examples dumps.
        Labels should be encoded from 0 (no activity) over 0.5 (first active bin) to 1. (maximum activity).

        Returns:
            batch: A np.ndarray holding the batch
            labels: A np.ndarray holding the labels
        """
        seq_tensors = []
        labels = []
        in_csv = self.traindata
        for i in range(self._opts._batchsize):
            try:
                """ Note that this is not shuffled! """
                name, label, seq_tensor, _, _ = self._process_csv(in_csv, return_name=True,
                                                                  encode_labels=False)
                seq_tensors.append(seq_tensor)
                labels.append(label)
            except IndexError: # catches error from csv_decoder
                # reopen the file:
                in_csv.close()
                # TODO: implement file shuffling when we reopen the file
                self.traindata = open(self._opts._traindata, 'r')
                in_csv = self.traindata
                """ redo """
                name, label, seq_tensor, _, _ = self._process_csv(in_csv, return_name=True,
                                                                  encode_labels=False)
                seq_tensors.append(seq_tensor)
                labels.append(label)

        # encode the labels "one-hot" although not really one hot, but rather with the passed score
        labels_tensor = []
        for i in labels:
            labels_tensor = [np.float32(l) for l in labels]
        del labels
        labels = np.stack(labels_tensor, axis=0) # [b, 1]
        #print(labels.shape)
        batch = np.stack(seq_tensors, axis=0)
        batch = np.expand_dims(batch, axis=-1) # [b, aa, w, 1]

        return batch, labels

    def generate_valid_batch(self, include_garbage=False):
        """Generate a batch of sequences form the valid set for inference.
        Draws samples from the valid set and generates a batch to infer the labels. As everything is fed into
        the same graph, we use the same kind of preprocessing as in generate_batch().

        This function is also used in the DeeProtein.generate_embedding().

        Args:
          include_garbage: A `bool`, if True, include garbage sequences into the valid batch (optional).
            Defaults to False

        Returns:
          batch: A `Tensor`, [batchsize, 20, windowlength, 1] holding the sequences batch.
        """
        seq_tensors = []
        in_csv = self.validdata
        if not include_garbage:
            for i in range(self._opts._batchsize):
                try:
                    """ Note that this is not shuffled! """
                    name, label, seq_tensor, _, _ = self._process_csv(in_csv, return_name=True,
                                                                      encode_labels=False)
                    self._update_embedding_dict(name, label)
                    seq_tensors.append(seq_tensor)
                except IndexError: # catches error from csv_decoder
                    # reopen the file:
                    in_csv.close()
                    # TODO: implement file shuffling when we reopen the file
                    self.validdata = open(self._opts._validdata, 'r')
                    in_csv = self.validdata
                    """ redo """
                    name, label, seq_tensor, _, _ = self._process_csv(in_csv, return_name=True,
                                                                      encode_labels=False)
                    self._update_embedding_dict(name, label)
                    seq_tensors.append(seq_tensor)

        #
        elif include_garbage:
            num_garbage = math.ceil(self._opts._batchsize * self.garbage_percentage)
            for i in range(self._opts._batchsize - num_garbage):
                try:
                    """ Note that this is not shuffled! """
                    name, label, seq_tensor, _, _ = self._process_csv(in_csv, return_name=True,
                                                                      encode_labels=False)
                    self._update_embedding_dict(name, label)
                    seq_tensors.append(seq_tensor)
                except IndexError: # catches error from csv_decoder
                    # reopen the file:
                    in_csv.close()
                    # TODO: implement file shuffling when we reopen the file
                    self.validdata = open(self._opts._validdata, 'r')
                    in_csv = self.validdata
                    """ redo """
                    name, label, seq_tensor, _, _ = self._process_csv(in_csv, return_name=True,
                                                                      encode_labels=False)
                    self._update_embedding_dict(name, label)
                    seq_tensors.append(seq_tensor)

            for i in range(num_garbage):
                name, seq_tensor, _, _ = self.generate_garbage_sequence(return_name=True)
                label = 'garbage'
                self._update_embedding_dict(name, label)
                seq_tensors.append(seq_tensor)

        batch = np.stack(seq_tensors, axis=0)
        batch = np.expand_dims(batch, axis=-1)
        return batch


class TFrecords_generator():
    """TFrecords_generator of the DeeProtein model.
    This class takes a splitted dataset (train, valid) and generates examples files as tf.Records files. This
    file format is required to train the model from the DeeProtein inputpipeline.

    Attributes:
        _opts: A `helpers.OptionHandler` holding the global options for the module.
        label_enc: A `sklearn.preprocessing.OneHotEncoder` used to encode the labels.
        AA_enc: A `sklearn.preprocessing.OneHotEncoder` used to encode the AA letters.
        mode: A `str`. Has to be one of ['one_hot_padded']
        traindata: A `fileObject` the file from which to train.
        validdata: A `fileObject` the file from which to evaluate.
        AA_to_id: A `dict` mapping single letter AAs to integers.
        class_dict: An `OrderedDict` mapping `str` labels to integers. The order is kept from the EC_file specifier in
          the config_dict.
        structure_dict: An `dict` mapping the structural information to an int.
        examples_per_file: A `int32`, defines the nr of sampels that is included in a tf.Records file.
        epochs: A `int32` defining the nr of epochs to train.
        curr_epoch: A `int32` counter for the epcohs.
        writer: A `fileObj` in which to write the log messages.
    """
    def __init__(self, optionhandler):
        self._opts = optionhandler
        self.label_enc = OneHotEncoder(n_values=self._opts._nclasses, sparse=False)
        self.AA_enc = 'where we put the encoder for the AAs'
        self.mode = self._opts._batchgenmode # one of ['window', 'bigbox', 'dynamic']
        self.traindata = open(self._opts._traindata, 'r')
        self.validdata = open(self._opts._validdata, 'r')
        self.AA_to_id = {}
        self.class_dict = {}
        self._get_class_dict()
        self.structure_dict = {}
        self.examples_per_file = 10000
        self.epochs = self._opts._numepochs
        self.curr_epoch = 0
        self.writer = 'where we put the writer'

        # get the structure_dict
        structure_forms = ['UNORDERED', 'HELIX', 'STRAND', 'TURN']
        assert len(structure_forms) == self._opts._structuredims-1
        for s in structure_forms:
            self.structure_dict[s] = len(self.structure_dict) + 1 #serve the 0 for NO INFORMATION

        if self.mode.startswith('one_hot'):
            AAs = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',
                   'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y',]

                   #'X']
            self.AA_enc = OneHotEncoder(n_values=self._opts._depth, sparse=False)
            if 'physchem' in self.mode:
                _hydro = [1.8, 2.5, -3.5, -3.5, 2.8,
                          -0.4, -3.2, 4.5, -3.9, 3.8,
                          1.9, -3.5, -1.6, -3.5, -4.5,
                          -0.8, -0.7, 4.2, -0.9, -1.3]
                _molarweight = [89.094, 121.154, 133.104, 147.131, 165.192,
                                75.067, 155.156, 131.175, 146.189, 131.175,
                                149.208, 132.119, 115.132, 146.146, 174.203,
                                105.093, 119.119, 117.148, 204.228, 181.191]
                _is_polar = lambda aa: 1 if aa in ['DEHKNQRSTY'] else 0
                _is_aromatic = lambda aa: 1 if aa in ['FWY'] else 0
                _has_hydroxyl = lambda aa: 1 if aa in ['ST'] else 0 #should we add TYR??
                _has_sulfur = lambda aa: 1 if aa in ['CM'] else 0

                for i, aa in enumerate(AAs):
                    self.AA_to_id[aa]  = {'id': len(self.AA_to_id),
                                          'hydro': _hydro[i],
                                          'molweight': _molarweight[i],
                                          'pol': _is_polar(aa),
                                          'arom': _is_aromatic(aa),
                                          'sulf': _has_sulfur(aa),
                                          'OH': _has_hydroxyl(aa)}
            else:
                for aa in AAs:
                    self.AA_to_id[aa] = len(self.AA_to_id)
                # get the inverse:
                self.id_to_AA = {}
                for aa, id in self.AA_to_id.items():
                    self.id_to_AA[id] = aa
                self.id_to_AA[42] = '_'

    def _get_class_dict(self):
        """
        This function generates the class-dict. The class dict stores the link between the index in the one-hot
        encoding and the class.
        """
        with open(self._opts._ecfile, "r") as ec_fobj:
            for line in ec_fobj:
                fields = line.strip().split()
                if fields[1].endswith('.csv'):  #TODO delete this when error is fixed
                    fields[1] = fields[1].rstrip('.csv')

                if self._opts._labels == 'EC':
                    self.class_dict[fields[1]] = {'id': len(self.class_dict),
                                                  'size': int(fields[0]),
                                                  }
                if self._opts._labels == 'GO':
                    self.class_dict[fields[1].split('_')[1]] = {'id': len(self.class_dict),
                                                  'size': int(fields[0]),
                                                  }

    def _csv_EC_decoder(self, in_csv):
        """Helper function for _process_csv().
        This function takes an in csv obj and iterates over the lines.

        Args:
          in_csv: `fileobject`, the incsv file object.
          encoded_labels: `bool`, wheter to return one-hot encoded labels or the label as string (optional).
            Defaults to True.

        Returns:
          name: A `str` holding the name of the sample sequence.
          seq: A `str` holding the sequence.
          seq: A `str` holding the label(s).
        """
        line = in_csv.readline()
        fields = line.strip().split(';')
        name = fields[0]
        seq = fields[1]
        if self._opts._labels == 'EC':
            if fields[3].endswith('.csv'):
                fields[3] = fields[3].rstrip('.csv')
            EC_str = fields[3]
            EC_CLASS = 0 if self._opts._inferencemode else self.class_dict[EC_str]['id']
            label = [[EC_CLASS]] # we need a 2D array
        elif self._opts._labels == 'GO':
            GO_str = fields[2]
            GOs = 0 if self._opts._inferencemode else fields[2].split(',')
            if GOs[0].endswith('.csv'):
                GOs = [go.rstrip('.csv') for go in GOs]
            label = [[self.class_dict[go]['id']] for go in GOs] # returns a 2D array
        # TODO add an assertion for mode
        structure_str = fields[3]
        return name, seq, label, structure_str

    def _seq2tensor(self, seq):
        """Helper function for _process_csv().
        Takes a sequence and encodes it onehot.

        Args:
          seq: `str`, The sequence to encode

        Returns:
          padded_seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos.
          length: A `Tensor` holding the length.
        """
        if self.mode == 'one_hot_padded':
            # first check if the sequence fits in the box:
            if len(seq) <= self._opts._windowlength:
                seq_matrix = np.ndarray(shape=(len(seq)), dtype=np.int32)
            # if sequence does not fit we clip it:
            else:
                seq_matrix = np.ndarray(shape=(self._opts._windowlength), dtype=np.int32)
            for i in range(len(seq_matrix)):
                seq_matrix[i] = self.AA_to_id[seq[i]]
            start_pos = 0 #because our sequence sits at the beginning of the box
            length = len(seq_matrix)  #true length (1 based)
            # now encode the sequence in one-hot
            oh_seq_matrix = np.reshape(self.AA_enc.fit_transform(np.reshape(seq_matrix,
                                                                            (1, -1))), (len(seq_matrix), 20))
            # pad the sequence to the boxsize:
            npad = ((0, self._opts._windowlength-length), (0, 0))
            padded_seq_matrix = np.pad(oh_seq_matrix, pad_width=npad, mode='constant', constant_values=0)
            padded_seq_matrix = np.transpose(padded_seq_matrix)
            del oh_seq_matrix, seq_matrix

            return padded_seq_matrix, start_pos, length #true length 1 based

        else:
            print("Error: MODE must be of 'one_hot_padded'")

    def _get_structure(self, structure_str, seq_length):
        """Encodes the 2ndary strucure of a sequence.
        Construct a One Hot Encoded Tensor with height = self._structure_dims, width = self._windowlength

        Args:
          structure_str: A `str` object holding the structural information for a sequence.
            The entry in the dataset.csv corresponds to the FT fields in the swissprot textfile download.
            Example format:
            [('TURN', '11', '14'), ('HELIX', '19', '27'), ('STRAND', '32', '36'), ('HELIX', '45', '54'),
            ('STRAND', '59', '69'), ('STRAND', '72', '80'), ('HELIX', '86', '96'), ('HELIX', '99', '112'),
            ('HELIX', '118', '123'), ('HELIX', '129', '131'), ('HELIX', '134', '143'), ('STRAND', '146', '149'),
            ('HELIX', '150', '156'), ('STRAND', '157', '159'), ('HELIX', '173', '182'), ('STRAND', '186', '189'),
            ('HELIX', '192', '194'), ('HELIX', '199', '211'), ('STRAND', '216', '221'), ('HELIX', '226', '239'),
            ('STRAND', '242', '246'), ('HELIX', '272', '275'), ('HELIX', '277', '279'), ('STRAND', '283', '285')]

        Returns:
          padded_structure_matrix: A `Tensor`, of shape [structure_dims, windowlength] holding the structure info.
        """
        # if there is info about the structure:
        if structure_str != '[]':
            # get an array of len length:
            structure = np.ones([seq_length])
            # modify the structure str:
            # TODO: Improve the super ugly hack with a proper regex
            structure_str = re.sub('[\'\[\]\(]', '', structure_str)
            structure_features = [j.strip(', ').split(', ') for j in structure_str.strip(')').split(')')]

            for ft in structure_features:
                # get the ID for the ft:
                id_to_write = self.structure_dict[ft[0]]
                start = int(ft[1])
                end = int(ft[2])
                for i in range(start, end+1):
                    structure[i] = id_to_write
            npad = ((0, self._opts._windowlength-seq_length))
            padded_structure_matrix = np.pad(structure, pad_width=npad, mode='constant', constant_values=0)
        else:
            # return only zeros if there is no information about the structure
            padded_structure_matrix = np.zeros([self._opts._windowlength])

        return padded_structure_matrix

    def _process_csv(self, queue):
        """Iterate over a dataset.csv and process the samples.

        Takes the path to a file and processes the samples a sample at a time whenever the function is called.

        Args:
          queue: `str` the file_path to the dataset.csv. The csv file should be in the form of:
            name,seq,labels.
          return_name: `bool` whether to return the sequence name or not. Defaults to True.
          encode_labels: `bool` whether to return the encoded labels or the raw str. Defaults to True.

        Returns:
          Depending on return_name either (if true):
          name: A `str` holding the sample name.
          label: A `Tensor` holding the one-hot encoded labels (depending on encode_labels, otherwise raw labels).
          seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos of the sequence in the window (defaults to 0).
          end_pos:  A `Tensor` holding the end pos of the sequence in the window.

          OR
          label: A `Tensor` holding the one-hot encoded labels (depending on encode_labels, otherwise raw labels).
          seq_matrix: A `Tensor` holding the one-hot encoded sequence.
          start_pos: A `Tensor` holding the start pos of the sequence in the window (defaults to 0).
          end_pos:  A `Tensor` holding the end pos of the sequence in the window.
        """
        _, seq, labels, structure_str = self._csv_EC_decoder(queue)
        seq_matrix, start_pos, length = self._seq2tensor(seq)
        try:
            structure_tensor = self._get_structure(structure_str, length)
        except KeyError:
            structure_tensor = 'no_stucture_defined'
        # encode the label one_hot:
        oh_label_tensor = self.label_enc.fit_transform(labels) # of shape [1, n_classes]
        classes = oh_label_tensor.shape[0]
        # open an array full of zeros to add the labels to
        oh_labels = np.zeros(self._opts._nclasses)
        for c in range(classes):
            oh_labels += oh_label_tensor[c]

        oh_labels = np.expand_dims(oh_labels, axis=0)

        return oh_labels, seq_matrix, structure_tensor, start_pos, length

    def generate_garbage_sequence(self):
        """Generate a garbage sequence.
        Generates a sequence full of garbage, e.g. a obviously non functional sequence. Used to test the robustness of
        the model precision.

        Args:
          return_name: A `bool`, if True return the garbage sequene along wih a generated name.

        Returns:
          name: A `str` holding the sample name (if return_name is True).
          padded_seq: A `Tensor` holding the one-hot encoded and padded (to windowlength) garbage sequence.
          label: A `Tensor` holding the one-hot encoded labels (depending on encode_labels, otherwise raw labels).
            The labels of a garbage sequence are a zeros Tensor.
          garbage_label: A `Tensor` holding the garbage label (binary, either 1 for garbage or 0 for valid sequence).
        """
        modes = ['complete_random', 'pattern', 'same']
        mode = modes[random.randint(0, 2)]

        # get the length of the protein
        length = random.randint(175, self._opts._windowlength-1)

        if mode == 'pattern':
            #print('pattern')
            # Generate a repetitive pattern of 5 AminoAcids to generate the prot
            # get a random nr of AAs to generate the pattern:
            AA_nr = random.randint(2, 5)
            # get an index for each AA in AA_nr
            idxs = []
            for aa in range(AA_nr):
                idx_found = False
                while not idx_found:
                    aa_idx = random.randint(0, 19)
                    if not aa_idx in idxs:
                        idxs.append(aa_idx)
                        idx_found = True
            reps = math.ceil(length/AA_nr)
            seq = reps * idxs
            length = len(seq)

        elif mode == 'complete_random':
            # print('complete_random')
            seq = []
            for aa in range(length):
                # get an idx for every pos in length:
                idx = random.randint(0, 19)
                seq.append(idx)

        elif mode == 'same':
            # print('ONE')
            AA = random.randint(0, 19)
            seq = length * [AA]

        label = np.zeros([self._opts._nclasses])
        label = np.expand_dims(label, axis=0)
        garbage_label = np.asarray([1])
        garbage_label = np.expand_dims(garbage_label, axis=0)
        oh_seq_matrix = np.reshape(self.AA_enc.fit_transform(np.reshape(seq, (1, -1))), (len(seq), 20))
        # pad the sequence to the boxsize:
        npad = ((0, self._opts._windowlength-length), (0, 0))
        padded_seq_matrix = np.pad(oh_seq_matrix, pad_width=npad, mode='constant', constant_values=0)
        padded_seq = np.transpose(padded_seq_matrix)
        return padded_seq, label, garbage_label

    def _bytes_feature(self, value):
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

    def _float_feature(self, value):
        return tf.train.Feature(bytes_list=tf.train.FloatList(value=[value]))

    def _int64_feature(self, value):
        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

    def example_to_TFrecords(self, is_train, garbage_percentage=0.2, structure=True):
        """Convert a dataset.csv into tf.Records format.
        This function reads the dataset files specified in the config dict and generates examples files in
        tf.Records format in the batches_dir folder specified in the config dict

        Args:
          is_train: A `bool` defining which dataset file to use (true: train, false: valid).
          garbage_percentage: A `float` defining the percentage of garbage to add to the tf.records files.
          structure: A `bool` whether to include structural information in the records or not.
        """
        include_garbage = False if garbage_percentage == 0 else True

        # determine how many files we need to write:
        if is_train:
            length_data_set = _count_lines(self._opts._traindata)
            batch_files_name = os.path.basename(self._opts._traindata) + \
                               'train_batch_{}'.format(str(self._opts._windowlength))
            print(batch_files_name)
            in_csv = self.traindata

        else:
            length_data_set = _count_lines(self._opts._validdata)
            batch_files_name = os.path.basename(self._opts._validdata) + \
                               'valid_batch_{}'.format(str(self._opts._windowlength))
            print(batch_files_name)
            in_csv = self.validdata

        files_to_write = np.int32(np.ceil(length_data_set*(1+garbage_percentage)
                                          / float(self.examples_per_file))) # write every thing twice

        for n in range(1, files_to_write+1):
            file_path = os.path.join(self._opts._batchesdir, batch_files_name) + '_' + str(n)
            self.writer = tf.python_io.TFRecordWriter(file_path)

            if structure:

                for i in range(self.examples_per_file):
                    if include_garbage and  i % int(1/garbage_percentage) == 0:
                        # print("garbage_seq")
                        seq_tensor, label, garbage_label = self.generate_garbage_sequence()
                        structure_label = np.zeros([self._opts._windowlength])

                        assert seq_tensor.shape == (self._opts._depth, self._opts._windowlength), \
                            "%s" % str(seq_tensor.shape)
                        assert label.shape == (1, self._opts._nclasses)
                        # convert the features to a raw string:
                        seq_raw = seq_tensor.tostring()
                        label_raw = label.tostring()
                        garbage_label_raw = garbage_label.tostring()
                        structure_label_raw = structure_label.tostring()

                        example = tf.train.Example(
                            features=tf.train.Features(feature={
                                'windowlength': self._int64_feature(self._opts._windowlength),
                                'structure_depth': self._int64_feature(self._opts._structuredims),
                                'depth': self._int64_feature(self._opts._depth),
                                'label_classes': self._int64_feature(self._opts._nclasses),
                                'seq_raw': self._bytes_feature(seq_raw),
                                'label_raw': self._bytes_feature(label_raw),
                                'garbage_label_raw': self._bytes_feature(garbage_label_raw),
                                'structure_label_raw': self._bytes_feature(structure_label_raw),
                            }))
                        self.writer.write(example.SerializeToString())
                    else:
                        # print("validseq")
                        try:
                            oh_labels, seq_tensor, structure_label, _, _ = self._process_csv(in_csv)

                        except IndexError: # catches error from csv_decoder -> reopen the file:
                            in_csv.close()
                            if is_train:
                                self.traindata = open(self._opts._traindata, 'r')
                                in_csv = self.traindata
                            else:
                                self.validdata = open(self._opts._validdata, 'r')
                                in_csv = self.validdata
                            oh_labels, seq_tensor, structure_label, _, _ = self._process_csv(in_csv)

                        garbage_label = np.asarray([0]) # NOT garbage
                        garbage_label = np.expand_dims(garbage_label, axis=0)

                        assert seq_tensor.shape == (self._opts._depth, self._opts._windowlength)
                        assert oh_labels.shape == (1, self._opts._nclasses)
                        # convert the features to a raw string:
                        seq_raw = seq_tensor.tostring()
                        label_raw = oh_labels.tostring()
                        garbage_label_raw = garbage_label.tostring()
                        structure_label_raw = structure_label.tostring()

                        example = tf.train.Example(
                            features=tf.train.Features(feature={
                                'windowlength': self._int64_feature(self._opts._windowlength),
                                'structure_depth': self._int64_feature(self._opts._structuredims),
                                'depth': self._int64_feature(self._opts._depth),
                                'label_classes': self._int64_feature(self._opts._nclasses),
                                'seq_raw': self._bytes_feature(seq_raw),
                                'label_raw': self._bytes_feature(label_raw),
                                'garbage_label_raw': self._bytes_feature(garbage_label_raw),
                                'structure_label_raw': self._bytes_feature(structure_label_raw),
                            }))
                        self.writer.write(example.SerializeToString())

            elif not structure:

                for i in range(self.examples_per_file):
                    if include_garbage and  i % int(1/garbage_percentage) == 0:
                        # print("garbage_seq")
                        assert seq_tensor.shape == (self._opts._depth, self._opts._windowlength),\
                            "%s" % str(seq_tensor.shape)
                        assert label.shape == (1, self._opts._nclasses)
                        # convert the features to a raw string:
                        seq_raw = seq_tensor.tostring()
                        label_raw = label.tostring()

                        example = tf.train.Example(
                            features=tf.train.Features(feature={
                                'windowlength': self._int64_feature(self._opts._windowlength),
                                'depth': self._int64_feature(self._opts._depth),
                                'label_classes': self._int64_feature(self._opts._nclasses),
                                'seq_raw': self._bytes_feature(seq_raw),
                                'label_raw': self._bytes_feature(label_raw),
                            }))
                        self.writer.write(example.SerializeToString())
                    else:
                        # print("validseq")
                        try:
                            oh_labels, seq_tensor, _, _, _ = self._process_csv(in_csv)

                        except IndexError: # catches error from csv_decoder -> reopen the file:
                            in_csv.close()
                            if is_train:
                                self.traindata = open(self._opts._traindata, 'r')
                                in_csv = self.traindata
                            else:
                                self.validdata = open(self._opts._validdata, 'r')
                                in_csv = self.validdata
                            oh_labels, seq_tensor, _, _, _ = self._process_csv(in_csv)

                        assert seq_tensor.shape == (self._opts._depth, self._opts._windowlength)
                        assert oh_labels.shape == (1, self._opts._nclasses)
                        # convert the features to a raw string:
                        seq_raw = seq_tensor.tostring()
                        label_raw = oh_labels.tostring()

                        example = tf.train.Example(
                            features=tf.train.Features(feature={
                                'windowlength': self._int64_feature(self._opts._windowlength),
                                'depth': self._int64_feature(self._opts._depth),
                                'label_classes': self._int64_feature(self._opts._nclasses),
                                'seq_raw': self._bytes_feature(seq_raw),
                                'label_raw': self._bytes_feature(label_raw),
                            }))
                        self.writer.write(example.SerializeToString())
            self.writer.close()

    def produce_train_valid(self):
        """Highlevel wrapper for the example_to_TF_records function."""
        assert self.mode.startswith('one_hot'), "mode must be 'one_hot_padded'"
        self.example_to_TFrecords(is_train=True, garbage_percentage=0, structure=False)
        self.example_to_TFrecords(is_train=False, garbage_percentage=0, structure=False)

def plot_histogram(log_file, save_dir):
    """Simple plotting function to plot a hist from a specified file containing counts per labels.

    Args:
        log_file: A `str` to the file containing the histogram data.
    """
    count_dict = {}
    with open(log_file, "r") as in_fobj:
        for line in in_fobj:
            pred_labels = line.strip().split()
            for label in pred_labels:
                try:
                    count_dict[label] += 1
                except KeyError:
                    count_dict[label] = 0
    bars = [count_dict[label] for label in count_dict.keys()]
    labels = [label for label in count_dict.keys()]
    set_style("whitegrid")
    fig, ax = plt.subplots()
    ax = barplot(x=bars, y=labels)
    fig.save(os.path.join(save_dir, 'negative_test.png'))


def _add_var_summary(var, name, collection=None):
    """Attaches a lot of summaries to a given tensor.

    Args:
      var: A `Tensor`, for which to calculate the summaries.
      name: `str`, the name of the Tensor.
      collection: `str` the collection to which to add the summary. Defaults to None.
    """
    with tf.name_scope(name):
        with tf.name_scope('summaries'):
            mean = tf.reduce_mean(var)
            tf.summary.scalar('mean', mean, collections=collection)
            with tf.name_scope('stddev'):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
            tf.summary.scalar('stddev', stddev, collections=collection)
            tf.summary.scalar('max', tf.reduce_max(var), collections=collection)
            tf.summary.scalar('min', tf.reduce_min(var), collections=collection)
            tf.summary.histogram('histogram', var, collections=collection)


def _variable_on_cpu(name, shape, initializer, trainable):
    """Helper function to get a variable stored on cpu.

    Args:
      name: A `str` holding the name of the variable.
      shape: An `Array` defining the shape of the Variable. For example: [2,1,3].
      initializer: The `tf.Initializer` to use to initialize the variable.
      trainable: A `bool` stating wheter the variable is trainable or not.

    Returns:
      A `tf.Variable` on CPU.
    """
    with tf.device('/cpu:0'): #TODO will this work?
        dtype = tf.float32
        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)
    #dtf.add_to_collection('CPU', var)
    return var

def softmax(X, theta = 1.0, axis = None):
    """Compute the softmax of each element along an axis of X.

    Args:
      X: `ND-Array`, Probably should be floats.
      theta: float parameter, used as a multiplier
        prior to exponentiation. Default = 1.0 (optional).
      axis: axis to compute values along. Default is the
        first non-singleton axis (optional).

    Returns:
    An `Array` of same shape as X. The result will sum to 1 along the specified axis.
    """
    # make X at least 2d
    y = np.atleast_2d(X)
    # find axis
    if axis is None:
        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)
    # multiply y against the theta parameter,
    y = y * float(theta)
    # subtract the max for numerical stability
    y = y - np.expand_dims(np.max(y, axis = axis), axis)
    # exponentiate y
    y = np.exp(y)
    # take the sum along the specified axis
    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)
    # finally: divide elementwise
    p = y / ax_sum
    # flatten if X was 1D
    if len(X.shape) == 1: p = p.flatten()
    return p

def untar(file):
    """Untar a file in the current wd.

    Args:
      file: A str specifying the filepath
    """
    try:
        tar = tarfile.open(fname)
        tar.extractall()
        tar.close()
    except:
        print('ERROR: File is not a .tar.gz, or does not exist.')



